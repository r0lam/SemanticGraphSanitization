{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "def compute_rouge_l(batch_data):\n",
    "    \"\"\"\n",
    "    输入为一个元组list，每一个元组是一个pred/gt对\n",
    "    Compute the average ROUGE-L F1 score for a batch of predictions and ground truths.\n",
    "\n",
    "    Args:\n",
    "        batch_data (list): A list of tuples where each tuple contains:\n",
    "            - prediction (str): The model's predicted text.\n",
    "            - ground_truth (str): The reference ground truth text.\n",
    "\n",
    "    Returns:\n",
    "        float: The average ROUGE-L F1 score over the batch.\n",
    "    \"\"\"\n",
    "    # Initialize the ROUGE evaluator with only ROUGE-L metric\n",
    "    rouge = Rouge(metrics=['rouge-l'])\n",
    "\n",
    "    # Separate predictions and ground truths\n",
    "    predictions = [pred for pred, gt in batch_data]\n",
    "    ground_truths = [gt for pred, gt in batch_data]\n",
    "    try:\n",
    "    # Compute ROUGE-L scores for the batch\n",
    "        scores = rouge.get_scores(predictions, ground_truths, avg=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # Extract the average ROUGE-L F1 score\n",
    "    rouge_l_f1 = scores['rouge-l']['f']\n",
    "\n",
    "    return rouge_l_f1\n",
    "\n",
    "def eval_samsum(data,evaled_model_name):\n",
    "    eval_tuple_list = [(data_i[f'{evaled_model_name}_res'], data_i['summary']) for data_i in data]\n",
    "    #eval_tuple_list = [(data_i[f'refine_res'], data_i['summary']) for data_i in data]\n",
    "    print(eval_tuple_list)\n",
    "    rougl = compute_rouge_l(eval_tuple_list)\n",
    "    print(rougl)\n",
    "    return rougl\n",
    "\n",
    "def eval_qnli(data,evaled_model_name):\n",
    "    right_num = 0\n",
    "    total = 0\n",
    "    for i in data:\n",
    "        #check\n",
    "        mapping = {\n",
    "            'entailment': 0,\n",
    "            'not entailment': 1,\n",
    "        }\n",
    "        if i[f'{evaled_model_name}_res'] not in ['entailment','not entailment']:\n",
    "            raise ValueError(\"QNLI数据集有问题\")\n",
    "\n",
    "        if i['label'] == mapping[i[f'{evaled_model_name}_res']]:\n",
    "            right_num = right_num+1\n",
    "            total = total+1\n",
    "        else:\n",
    "            total = total+1\n",
    "    print(\"acc: \"+str(right_num/total))\n",
    "    return right_num/total\n",
    "\n",
    "def eval_reclor(data,evaled_model_name):\n",
    "    right_num = 0\n",
    "    total = 0\n",
    "    map_dict = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    for i in data:\n",
    "        if i[f'{evaled_model_name}_res'] in ['A','B','C',\"D\"]:\n",
    "            if i[f'{evaled_model_name}_res'] == chr(i['label']+65):\n",
    "                right_num = right_num+1\n",
    "                total = total+1                \n",
    "            else:\n",
    "                total = total+1\n",
    "        else:\n",
    "            continue\n",
    "    print(\"acc: \"+str(right_num/total))\n",
    "    return right_num/total\n",
    "\n",
    "def eval_one_file(data,dataset_name, evaled_model_name):\n",
    "    print(f'开始评估数据集: {evaled_model_name}-{dataset_name}')\n",
    "    if dataset_name == \"SAMsum\":\n",
    "        return eval_samsum(data,evaled_model_name)\n",
    "    elif dataset_name == \"QNLI\":\n",
    "        return eval_qnli(data,evaled_model_name)\n",
    "    elif dataset_name == \"reClor\":\n",
    "        return eval_reclor(data,evaled_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_pii_leakage(text,word_set):\n",
    "\n",
    "    #words_in_text = re.findall(r'\\b\\w+\\b', text)\n",
    "    matched_words = []\n",
    "    for word in word_set:\n",
    "        if word in text:\n",
    "            matched_words.append(word)\n",
    "    #matched_words = {word for word in words_in_text if word in word_set}\n",
    "    return matched_words,len(matched_words)\n",
    "\n",
    "def routine_san(data,task_name):\n",
    "    real_pii_path = f\"./tak/extract_out/{task_name}/{task_name}.json\"\n",
    "    #data_path = f\"./eval_data/{privacy_method}/{task_name}/{task_name}.json\"\n",
    "    # with open(data_path, 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    with open(real_pii_path, 'r') as f:\n",
    "        real_pii = json.load(f)\n",
    "    pii_list = []\n",
    "    for key in real_pii:\n",
    "        pii_list = pii_list+real_pii[key]\n",
    "    pii_set = set(pii_list)\n",
    "    total_leakage = 0\n",
    "    corpus = \"\"\n",
    "    for data_i in data:\n",
    "        if task_name == \"SAMsum\":\n",
    "            corpus = f\"{corpus} {data_i['dialogue']}\"\n",
    "        if task_name == \"QNLI\":\n",
    "            corpus = f\"{corpus} {data_i['text1']} {data_i['text2']}\"\n",
    "        if task_name == \"reClor\":\n",
    "            corpus = f\"{corpus} {data_i['context']} {data_i['question']} {str(data_i['answers'])}\"\n",
    "    leak_set,leak_num = find_pii_leakage(corpus,pii_set)\n",
    "    total_leakage = leak_num\n",
    "    return 1-(total_leakage/len(pii_set))\n",
    "\n",
    "def routine(task_name, privacy_method,eps):\n",
    "    real_pii_path = f\"./tak/extract_out/{task_name}/{task_name}.json\"\n",
    "    data_path = f\"./eval_data/{privacy_method}/{task_name}/eps_{str(eps)}/{task_name}.json\"\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    with open(real_pii_path, 'r') as f:\n",
    "        real_pii = json.load(f)\n",
    "    \n",
    "    pii_list = []\n",
    "    for key in real_pii:\n",
    "        pii_list = pii_list+real_pii[key]\n",
    "    pii_set = set(pii_list)\n",
    "    total_leakage = 0\n",
    "    corpus = \"\"\n",
    "    for data_i in data:\n",
    "        if task_name == \"SAMsum\":\n",
    "            corpus = f\"{corpus} {data_i['dialogue']}\"\n",
    "        if task_name == \"QNLI\":\n",
    "            corpus = f\"{corpus} {data_i['text1']} {data_i['text2']}\"\n",
    "        if task_name == \"reClor\":\n",
    "            corpus = f\"{corpus} {data_i['context']} {data_i['question']} {str(data_i['answers'])}\"\n",
    "\n",
    "\n",
    "    leak_set,leak_num = find_pii_leakage(corpus,pii_set)\n",
    "    total_leakage = leak_num\n",
    "    return 1-(total_leakage/len(pii_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SAMsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_san_data_path_list = [\n",
    "    # f\"./dataset/code_sanitization/SPO/0.3_0.1/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.5_0.1/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.7_0.1/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.9_0.1/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.5_0.3/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.7_0.3/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.9_0.3/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.7_0.5/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.9_0.5/graph/{dataset_name}.json\",\n",
    "    # f\"./dataset/code_sanitization/SPO/0.9_0.7/graph/{dataset_name}.json\",\n",
    "\n",
    "    #无隐私处理+语义图\n",
    "    #f\"./dataset/no/SPO/0.3_0.1/graph/{dataset_name}.json\"\n",
    "    f\"E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\{dataset_name}\\\\eps_1.0\\\\{dataset_name}.json\",\n",
    "    f\"E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\{dataset_name}\\\\eps_2.0\\\\{dataset_name}.json\",\n",
    "    f\"E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\{dataset_name}\\\\eps_3.0\\\\{dataset_name}.json\",\n",
    "    f\"E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\{dataset_name}\\\\eps_4.0\\\\{dataset_name}.json\",\n",
    "    f\"E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\{dataset_name}\\\\eps_5.0\\\\{dataset_name}.json\",\n",
    "    f\"E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\{dataset_name}\\\\eps_6.0\\\\{dataset_name}.json\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\QNLI\\\\eps_1.0\\\\SAMsum.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m res_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m our_san_data_path_list:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      5\u001b[0m     res_list\u001b[38;5;241m.\u001b[39mappend((path,eval_one_file(data, dataset_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m),routine_san(data,dataset_name)))\n",
      "File \u001b[1;32me:\\conda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\desktop\\\\prompt-privacy-final\\\\eval_data\\\\icl\\\\QNLI\\\\eps_1.0\\\\SAMsum.json'"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "for path in our_san_data_path_list:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    res_list.append((path,eval_one_file(data, dataset_name, \"gpt-3.5-turbo\"),routine_san(data,dataset_name)))\n",
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
